parameters:
    max_number_of_urls: 1_000_000_000 # one billion
    
    url_collector:
        number_of_threads: 50
        save_to_file_frequency: 15 # !seconds
    
    metadata_collector:
        number_of_threads: 20
        save_to_file_frequency: 15 # !seconds 
    
    facedata_collector:
        storage_cap              : 100 # number of good videos (generally 10Mb of screenshots per video)
        number_of_sample_frames  : 10
        number_of_frames_needed  : 50.percent * number_of_sample_frames
        required_face_size       : 25.percent * Video.frame_height
        similarity_rejection_rate: 50.percent * number_of_frames_needed  # if one face is repeated (exact coords) 50% of the time (ex: 5 frames) then reject video as just an image


(using_atk_version): 1.1.0
(project):
    name: Expressions Dataset
    description: A Dataset of video clips for facial expressions
        
    (commands):
        # main commands
        add_facedata: _ run ruby smart_scraper/add_facedata.rb
        # setup
        (setup): ruby scripts/install.rb
        download_labeled_dataset: python3 collect_from_csv/download.py videos.csv
        # docker stuff
        build: ruby scripts/build.rb
        run: ruby scripts/run.rb
        edit: ruby scripts/edit.rb
        remove: ruby scripts/remove.rb

    (paths):
        all_urls: ./database/urls.json
        ruby_tools: ./toolbox/tools.rb
        python_tools: ./toolbox/tools.py
        face_detector: ./face_detection/face_detection.py
        raised_eyebrows_videos: collect_from_csv/clips.nosync/raised_eyebrows
        video_source: ./collect_from_csv/videos.csv
        adblock: ./smart_scraper/adblock/3.54.0_0
        filtered_videos: ./database/filtered_videos
        shape_predictor_68_face_landmarks: ./face_detection/shape_predictor_68_face_landmarks.dat
        font: ./iosevka-custom-regular.ttf
        dockerfiles: ./project_bin/dockerfiles/
        project_bin: ./project_bin
        url_collector_script: ./smart_scraper/url_collector.rb
        add_metadata_script: ./smart_scraper/add_metadata.rb
    
    dependencies:
        atk: 0.0.1
        ruby: 2.5.0
        python: 3.7.3
        ffmpeg: 4.1.4
        chromedriver: 76.0.3809
        imagemagick: 6.9.10-66
        youtube-dl: 2020.01.24
        # {pip3: 19.0.3}:
        #     youtube-dl: 2019.07.27
        #     ruamel.yaml: 0.16.0
        #     dlib: 19.17.0
        #     opencv-python: 4.1.1.26 
        #     scikit-learn: 0.21.3
        #     # pandas: 0.25.2
        # {gem: 3.0.4}:
        #     zlib: 1.1.0
        #     atk_toolbox: 0.0.136
        #     selenium-webdriver: 3.142.3
        #     webdrivers: 4.1.2
        #     colorize: 0.8.1
        #     activesupport: 5.2.3
        #     nokogiri: 1.10.4
        #     statistics2: 0.54.0

expressions:
    looking_at_source:
        number_of_examples: 0
        number_of_urls: 0
    slouching:
        number_of_examples: 0
        number_of_urls: 0
    chin_down:
        number_of_examples: 0
        number_of_urls: 0
    long_stare:
        number_of_examples: 0
        number_of_urls: 0
    squinting:
        number_of_examples: 1
        number_of_urls: 1
    o_shaped_lips:
        number_of_examples: 1
        number_of_urls: 1
    lean_forward:
        number_of_examples: 2
        number_of_urls: 2
    back_head_touch:
        number_of_examples: 6
        number_of_urls: 4
    forehead_touch:
        number_of_examples: 9
        number_of_urls: 6
    head_shake:
        number_of_examples: 9
        number_of_urls: 6
    pressed_eyebrows:
        number_of_examples: 9
        number_of_urls: 7
    head_nod:
        number_of_examples: 10
        number_of_urls: 6
    raised_eyebrows:
        number_of_examples: 14
        number_of_urls: 9
    chin_touch:
        number_of_examples: 17
        number_of_urls: 9
    blink:
        number_of_examples: 18
        number_of_urls: 11

common non-recorded expression:
  - smiles
  - sneer (muscles around nose)
  - hand gestures of various types
  - eye motions (thinking eyes)
  - full head/neck motions:
      - titing head to a side
      - pushing head forward or backwards
      - very small very quick tilt to one side ("well at least *that* worked")
  - the width of the eyes
  - shrug

challenges:
  - A bit discouraging with how non-binary expressions are. Forcing them into categories feels inaccurate
  - video contents:
      - multiple faces (very hard to get one face with genuine expressions)
      - people not directly facing the camera
      - glasses, headphones, hats, microphones
      - shaky video
      - lighting
      - |
        single faces are generally people actively talking or interviewing, not being passive.
        People who are passive are generally participating in something else, and usually that means other faces are on screen
        This means very little
      - face touching
      - neck touching
      - slouching/hunching

  - expressions often happen in well under 1 second
  - suprisingly hard to tell for some expressions. Some people/lighting makes it obvious and others make it very difficult
  - there's no great definition of an expression




todo:
    - make the exec use an ENV variable to know the path of the current project OR have it reverse engineer the path from the dockerfile image name
    - pass the directory through the exec:
        - get the relative dir
        - pass it as the workspace argument
    - maybe make recursive container calls (ffmpeg container call to ffmpeg) more efficient by not creating a new container
    - re-feeder: add priotities to which videos to explore next
    - stage 1:
        - fix the STD output from docker
    - stage 2:
        - dockerize it
        - make it where reloading is not an issue
    - stage 3:
        - dockerize it
        - fix the hangup issue for certain videos
    - stage 4 segment recognition:
        - use the facial expression recognition to add information to videos
    - check if the `urls = urls.merge(new_urls) { |key, v1, v2| v1 }` is causing race condition problems
    - Database:
        - have an init with username+password
        - have a superuser API with:
            - ability to add/remove approved keys
            - ability to create/remove databases (collections)
            - ability to change passwords
        - have an API library for:
            - connecting to a database with an approved key
            - set the value for a key
            - get the value for a key
            - check if key exists
            - filter all keys/values for a specific pattern
    - old:
        - clean up the Video class, make sure videos are only loaded once
        - add a check (or filter) to prevent the static images being counted as videos
        - create a ruby method for downloading youtube videos, and managing disk space, deleting the oldest videos
false negatives:
    - j4GnypVUfBw
    - 6mud7S8cBYM
    - miQoYrtIpTg
plan:
    - create a mongoDB database
    - have a server constantly adding urls
    - have a server constantly adding metadata to the URLs
    - have a server constantly downloading and running facial recognition on the URLs


old: |

    todo
    - use frame padding to deal with missing frames
    - change the features to be looking at differences for all lookback frames
    - small draft current approach
    - like a paper:
        - define topic, 
        - explain why solution would be useful, 
        - the approach
        - data set
        - tools (SVM)
        - feature development
        - results/performance
            - include the problem with missed frames and starting frames
        - conclusions

    todo later
    - label more training data using random (not 10th frames)
    - create an SVN for every 10% to get a 0-10 output
    - save the labelled data to JSON to a file
    - adjust video labeller GUI
    - iterate over labelled video frames
    - generate images for randomly selected frames
    - ask for the label, and record it
    - switch to better facial recognition
    - create a system for parameter optimization

    future plans
    - create a polygon for wrikle detection

    done
    - use average eye height instead of top of eye
    - create a way to measure the score compared to the hand picked score
    - create measure for mouth openness
    - include eye openness
    - use nose as vector
    - favor the maximum
    - have App show labels on hover



    1. topic: eyebrow raising
    2. dataset:
    - record self
    - videos from youtube
    3. Facial landmark
    -  add landmarks
    4. labels
    - label frames as percent for eyebrow rasining
    5. training
    - input = sequence of facial landmarks 
        - extract features
        - reduce the dimensionality to just the distance for each eye
        - try normalizing based on eye width or based on face height
        - auto ML tools, plug in parameters to
    - use an SVM 
    - output = true or false


    talk to jiang about
    - issue with particular thresholds not having enough
    - performance, 71%, 30 minutes of processing
    - how to affectively evaluate?
    - answer: create a video with the prediction overlayed
    - improving performance
    - more data, or method for visulizing basis for it's decision
    - no prediction until after 10th frame
    - trouble with certain frames
    - improving face-prediction method
    - improving the optimization by having continuous output and a more strict measurement (true/false is a 50% guess)
