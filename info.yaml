parameters:
    max_number_of_urls: 1_000_000_000 # one billion
    
    url_collector:
        number_of_threads: 50
        save_to_file_frequency: 15 # !seconds
    
    add_metadata:
        number_of_threads: 20
        save_to_file_frequency: 15 # !seconds 
    
    add_facedata:
        max_video_duration       : 8000 # !seconds
        min_video_duration       : 100 # !seconds
        storage_cap              : 200 # number of good videos (generally 10Mb of screenshots per video)
        number_of_sample_frames  : 10
        number_of_frames_needed  : 50.percent * number_of_sample_frames
        required_face_size       : 25.percent * Video.frame_height
        similarity_rejection_rate: 50.percent * number_of_frames_needed  # if one face is repeated (exact coords) 50% of the time (ex: 5 frames) then reject video as just an image
    
    database:
        port: 3000
        url: http://127.0.0.1:3000
            


(using_atk_version): 1.1.0
(project):
    name: Expressions Dataset
    description: A Dataset of video clips for facial expressions
        
    (commands):
        # main commands
        url_collector: ruby scripts/run.rb ruby smart_scraper/url_collector.rb
        add_facedata: ruby scripts/run.rb ruby smart_scraper/add_facedata.rb
        add_metadata: ruby scripts/run.rb ruby smart_scraper/add_metadata.rb
        recog: ruby scripts/run.rb python ./toolbox/face_tools/recognition/Face_Recognition.py
        detection: ruby scripts/run.rb python ./toolbox/face_tools/expressions/Facial_expressions_detection.py
        # setup
        (setup): ruby 'scripts/(setup).rb'
        download_labeled_dataset: python3 collect_from_csv/download.py videos.csv
        # docker stuff
        build: ruby scripts/build.rb
        run: ruby scripts/run.rb
        edit: ruby scripts/edit.rb
        remove: ruby scripts/remove.rb
        convert_name: ruby scripts/convert_name.rb
        start_x11: open -a XQuartz --hide;socat TCP-LISTEN:6000,reuseaddr,fork UNIX-CLIENT:\"$DISPLAY\" & ; echo "x11 started"
        start_database: ruby scripts/start_database.rb

    (paths):
        all_urls: ./database/urls.json
        new_data: ./database/new_data.json
        ruby_tools: ./toolbox/tools.rb
        python_tools: ./toolbox/tools.py
        database_api: ./toolbox/database_api.rb
        url_collector_script: ./smart_scraper/url_collector.rb
        add_metadata_script: ./smart_scraper/add_metadata.rb
        database_data: ./database/db.large
        # system
        project_bin: ./project_bin
        dockerfiles: ./project_bin/dockerfiles/
        project_dir_file: ./.project_dir.nosync.txt  # dont change unless you also change the project/bin exec
        # face tools
        face_detector: ./toolbox/face_tools/bounding_boxes/face_detection.py
        # working
        test_video: ./toolbox/face_tools/recognition/video1.mp4
        shape_predictor_68_face_landmarks: ./toolbox/face_tools/68_landmarks/shape_predictor_68_face_landmarks.dat
        haarcascade_frontalface_default: ./toolbox/face_tools/bounding_boxes/haarcascade_frontalface_default.xml
        test_model.t7: ./toolbox/face_tools/expressions/test_model.t7
        feature_file.npz: ./toolbox/face_tools/recognition/feature_file.npz
        emotion_model: './toolbox/face_tools/expressions/emotion_model' # probably a folder to save the emotion models
        mtcnn_face_model: './toolbox/face_tools/recognition/recognition/face_deploy.prototxt'
        mtcnn_face_weights: './toolbox/face_tools/recognition/recognition/85_accuracy.caffemodel'
        caffe_model_path: "./toolbox/face_tools/recognition/detection/"
        recognition_checkpoint_state: './toolbox/face_tools/recognition/expressions/models'
        expression_checkpoint_state: './toolbox/face_tools/recognition/expressions/ckpt'
        GenFeature_input_folder: ./database/user_images/
        # old
        raised_eyebrows_videos: collect_from_csv/clips.nosync/raised_eyebrows
        video_source: ./collect_from_csv/videos.csv
        adblock: ./smart_scraper/adblock/3.54.0_0
        filtered_videos: ./database/filtered_videos


expressions:
    looking_at_source:
        number_of_examples: 0
        number_of_urls: 0
    slouching:
        number_of_examples: 0
        number_of_urls: 0
    chin_down:
        number_of_examples: 0
        number_of_urls: 0
    long_stare:
        number_of_examples: 0
        number_of_urls: 0
    squinting:
        number_of_examples: 1
        number_of_urls: 1
    o_shaped_lips:
        number_of_examples: 1
        number_of_urls: 1
    lean_forward:
        number_of_examples: 2
        number_of_urls: 2
    back_head_touch:
        number_of_examples: 6
        number_of_urls: 4
    forehead_touch:
        number_of_examples: 9
        number_of_urls: 6
    head_shake:
        number_of_examples: 9
        number_of_urls: 6
    pressed_eyebrows:
        number_of_examples: 9
        number_of_urls: 7
    head_nod:
        number_of_examples: 10
        number_of_urls: 6
    raised_eyebrows:
        number_of_examples: 14
        number_of_urls: 9
    chin_touch:
        number_of_examples: 17
        number_of_urls: 9
    blink:
        number_of_examples: 18
        number_of_urls: 11

tools to research: 
    reinforcement learning: rlpyt https://github.com/astooke/rlpyt


common non-recorded expression:
  - smiles
  - sneer (muscles around nose)
  - hand gestures of various types
  - eye motions (thinking eyes)
  - full head/neck motions:
      - titing head to a side
      - pushing head forward or backwards
      - very small very quick tilt to one side ("well at least *that* worked")
  - the width of the eyes
  - shrug

lstm-plan:
    data:
        - video object created
        - video is pulled from source
        - transcription data and


todo:
    - start a labelling pipeline:
        - every video in the download folder gets labelled frame-for-frame
    - get the emotion detection working
    - make the docker database use ENV variables instead of hardcoded paths
    - make the exec use an ENV variable to know the path of the current project OR have it reverse engineer the path from the dockerfile image name
    - maybe make recursive container calls (ffmpeg container call to ffmpeg) more efficient by not creating a new container
    - re-feeder: add priotities to which videos to explore next
    - stage 2:
        - make it where reloading is not an issue
        - make it killable with a single ^C (right now the docker container must be killed manually)
        - fix the weird shifted output (maybe a multithreading issue)
    - stage 3:
        - fix the hangup issue for certain videos
    - stage 4 segment recognition:
        - use the facial expression recognition to add information to videos
    - check if the `urls = urls.merge(new_urls) { |key, v1, v2| v1 }` is causing race condition problems
    - Docker:
        - get matplotlib working through docker
        - get camera feed working through docker
        - get nvidia docker working
        - figure out networking better
    - Database:
        - setup the sample/limit function
        - run command:
            - should add the info.nosync.yaml as a volume
            - should add the username/password functionality
            - add ability to change database/collections
        - refine and then publish the docker container once express_server/index.js is done
    - old:
        - clean up the Video class, make sure videos are only loaded once
        - add a check (or filter) to prevent the static images being counted as videos
        - create a ruby method for downloading youtube videos, and managing disk space, deleting the oldest videos

old: |

    todo
    - use frame padding to deal with missing frames
    - change the features to be looking at differences for all lookback frames
    - small draft current approach
    - like a paper:
        - define topic, 
        - explain why solution would be useful, 
        - the approach
        - data set
        - tools (SVM)
        - feature development
        - results/performance
            - include the problem with missed frames and starting frames
        - conclusions

    todo later
    - label more training data using random (not 10th frames)
    - create an SVN for every 10% to get a 0-10 output
    - save the labelled data to JSON to a file
    - adjust video labeller GUI
    - iterate over labelled video frames
    - generate images for randomly selected frames
    - ask for the label, and record it
    - switch to better facial recognition
    - create a system for parameter optimization

    future plans
    - create a polygon for wrikle detection

    done
    - use average eye height instead of top of eye
    - create a way to measure the score compared to the hand picked score
    - create measure for mouth openness
    - include eye openness
    - use nose as vector
    - favor the maximum
    - have App show labels on hover



    1. topic: eyebrow raising
    2. dataset:
    - record self
    - videos from youtube
    3. Facial landmark
    -  add landmarks
    4. labels
    - label frames as percent for eyebrow rasining
    5. training
    - input = sequence of facial landmarks 
        - extract features
        - reduce the dimensionality to just the distance for each eye
        - try normalizing based on eye width or based on face height
        - auto ML tools, plug in parameters to
    - use an SVM 
    - output = true or false


    talk to jiang about
    - issue with particular thresholds not having enough
    - performance, 71%, 30 minutes of processing
    - how to affectively evaluate?
    - answer: create a video with the prediction overlayed
    - improving performance
    - more data, or method for visulizing basis for it's decision
    - no prediction until after 10th frame
    - trouble with certain frames
    - improving face-prediction method
    - improving the optimization by having continuous output and a more strict measurement (true/false is a 50% guess)
